{
 "className": "io.deephaven.db.tables.utils.TableManagementTools",
 "methods": {
  "deleteTable": "Deletes a table on disk.\n\n:param path: (java.io.File) - path to delete",
  "readTable": "Reads in a table from disk.\n\n*Overload 1*  \n  :param path: (java.io.File) - table location\n  :param tableDefinition: (io.deephaven.db.tables.TableDefinition) - table definition\n  :return: (io.deephaven.db.tables.Table) table\n  \n*Overload 2*  \n  :param path: (java.io.File) - table location\n  :return: (io.deephaven.db.tables.Table) table",
  "writeParquetTables": "Writes tables to disk in parquet format under a given destinations.  If you specify grouping columns, there\n must already be grouping information for those columns in the sources.  This can be accomplished with\n .by(<grouping columns>).ungroup() or .sort(<grouping column>).\n\n:param sources: (io.deephaven.db.tables.Table[]) - The tables to write\n:param tableDefinition: (io.deephaven.db.tables.TableDefinition) - The common schema for all the tables to write\n:param codecName: (org.apache.parquet.hadoop.metadata.CompressionCodecName) - Compression codec to use.  The only supported codecs are\n                        CompressionCodecName.SNAPPY and CompressionCodecName.UNCOMPRESSED.\n:param destinations: (java.io.File[]) - The destinations path\n:param groupingColumns: (java.lang.String[]) - List of columns the tables are grouped by (the write operation will store the grouping info)",
  "writeTable": "Write out a table to disk.\n\n*Overload 1*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param destDir: (java.lang.String) - destination\n  \n*Overload 2*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param destDir: (java.lang.String) - destination\n  :param storageFormat: (io.deephaven.db.tables.utils.TableManagementTools.StorageFormat) - Format used for storage\n  \n*Overload 3*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param definition: (io.deephaven.db.tables.TableDefinition) - table definition.  Will be written to disk as given.\n  :param destDir: (java.io.File) - destination\n  :param storageFormat: (io.deephaven.db.tables.utils.TableManagementTools.StorageFormat) - Format used for storage\n  \n*Overload 4*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param destDir: (java.io.File) - destination\n  \n*Overload 5*  \n  :param sourceTable: (io.deephaven.db.tables.Table) - source table\n  :param destDir: (java.io.File) - destination\n  :param storageFormat: (io.deephaven.db.tables.utils.TableManagementTools.StorageFormat) - Format used for storage",
  "writeTables": "Write out tables to disk.\n\n*Overload 1*  \n  :param sources: (io.deephaven.db.tables.Table[]) - source tables\n  :param tableDefinition: (io.deephaven.db.tables.TableDefinition) - table definition\n  :param destinations: (java.io.File[]) - destinations\n  \n*Overload 2*  \n  :param sources: (io.deephaven.db.tables.Table[]) - source tables\n  :param tableDefinition: (io.deephaven.db.tables.TableDefinition) - table definition\n  :param destinations: (java.io.File[]) - destinations\n  :param storageFormat: (io.deephaven.db.tables.utils.TableManagementTools.StorageFormat) - Format used for storage"
 },
 "path": "io.deephaven.db.tables.utils.TableManagementTools",
 "text": "Tools for managing and manipulating tables on disk.\n\n Most users will need TableTools and not TableManagementTools.",
 "typeName": "class"
}